#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TensorFlow Serving å®¢æˆ·ç«¯ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•è°ƒç”¨éƒ¨ç½²åœ¨ TensorFlow Serving ä¸Šçš„ Wide & Deep CTR æ¨¡å‹
"""

import json
import requests
import numpy as np
from typing import Dict, Any, List, Optional


class WideDeepServingClient:
    """Wide & Deep æ¨¡å‹ TensorFlow Serving å®¢æˆ·ç«¯"""
    
    def __init__(self, host: str = "localhost", port: int = 8501, model_name: str = "wide_deep_ctr"):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            host: TensorFlow Serving ä¸»æœºåœ°å€
            port: REST API ç«¯å£ (é»˜è®¤ 8501)
            model_name: æ¨¡å‹åç§°
        """
        self.base_url = f"http://{host}:{port}"
        self.model_name = model_name
        self.predict_url = f"{self.base_url}/v1/models/{model_name}:predict"
        self.model_url = f"{self.base_url}/v1/models/{model_name}"
    
    def check_model_status(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        
        Returns:
            æ¨¡å‹çŠ¶æ€ä¿¡æ¯
        """
        try:
            response = requests.get(self.model_url)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}
    
    def get_model_metadata(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹å…ƒæ•°æ®
        
        Returns:
            æ¨¡å‹å…ƒæ•°æ®ä¿¡æ¯
        """
        try:
            response = requests.get(f"{self.model_url}/metadata")
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}
    
    def predict(
        self,
        wide_features: List[float],
        deep_features: List[float],
        query_hash: int,
        doc_hash: int,
        position_group: int,
        model_version: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        å•æ ·æœ¬é¢„æµ‹
        
        Args:
            wide_features: Wide ç‰¹å¾å‘é‡ (6ç»´)
            deep_features: Deep ç‰¹å¾å‘é‡ (8ç»´)
            query_hash: æŸ¥è¯¢å“ˆå¸Œå€¼ (0-999)
            doc_hash: æ–‡æ¡£å“ˆå¸Œå€¼ (0-999)
            position_group: ä½ç½®åˆ†ç»„ (0-2)
            model_version: æ¨¡å‹ç‰ˆæœ¬å· (å¯é€‰)
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "instances": [
                {
                    "wide": wide_features,
                    "deep": deep_features,
                    "query_hash": query_hash,
                    "doc_hash": doc_hash,
                    "position_group": position_group
                }
            ]
        }
        
        # æ„å»º URL
        url = self.predict_url
        if model_version:
            url = f"{self.base_url}/v1/models/{self.model_name}/versions/{model_version}:predict"
        
        try:
            response = requests.post(url, json=request_data)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}
    
    def batch_predict(
        self,
        samples: List[Dict[str, Any]],
        model_version: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«:
                - wide: Wide ç‰¹å¾å‘é‡
                - deep: Deep ç‰¹å¾å‘é‡
                - query_hash: æŸ¥è¯¢å“ˆå¸Œå€¼
                - doc_hash: æ–‡æ¡£å“ˆå¸Œå€¼
                - position_group: ä½ç½®åˆ†ç»„
            model_version: æ¨¡å‹ç‰ˆæœ¬å· (å¯é€‰)
        
        Returns:
            æ‰¹é‡é¢„æµ‹ç»“æœ
        """
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "instances": samples
        }
        
        # æ„å»º URL
        url = self.predict_url
        if model_version:
            url = f"{self.base_url}/v1/models/{self.model_name}/versions/{model_version}:predict"
        
        try:
            response = requests.post(url, json=request_data)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}


def generate_sample_features() -> Dict[str, Any]:
    """ç”Ÿæˆç¤ºä¾‹ç‰¹å¾ç”¨äºæµ‹è¯•"""
    return {
        "wide": [1.0, 0.5, 0.8, 0.3, 0.1, 0.2],  # 6ç»´ Wide ç‰¹å¾
        "deep": [10.0, 5.0, 20.0, 3.0, 8.0, 0.5, 0.6, 0.1],  # 8ç»´ Deep ç‰¹å¾
        "query_hash": np.random.randint(0, 1000),  # æŸ¥è¯¢å“ˆå¸Œ
        "doc_hash": np.random.randint(0, 1000),  # æ–‡æ¡£å“ˆå¸Œ
        "position_group": np.random.randint(0, 3)  # ä½ç½®åˆ†ç»„
    }


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®¢æˆ·ç«¯ä½¿ç”¨"""
    print("=" * 60)
    print("ğŸš€ TensorFlow Serving Wide & Deep å®¢æˆ·ç«¯ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = WideDeepServingClient(
        host="localhost",
        port=8501,
        model_name="wide_deep_ctr"
    )
    
    # 1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    print("\nğŸ“Š æ£€æŸ¥æ¨¡å‹çŠ¶æ€...")
    status = client.check_model_status()
    print(json.dumps(status, indent=2))
    
    if "error" in status:
        print("\nâŒ æ— æ³•è¿æ¥åˆ° TensorFlow Servingï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
        print("   å¯åŠ¨å‘½ä»¤: docker run -p 8501:8501 -p 8500:8500 wide-deep-serving")
        return
    
    # 2. è·å–æ¨¡å‹å…ƒæ•°æ®
    print("\nğŸ“‹ è·å–æ¨¡å‹å…ƒæ•°æ®...")
    metadata = client.get_model_metadata()
    print(json.dumps(metadata, indent=2))
    
    # 3. å•æ ·æœ¬é¢„æµ‹
    print("\nğŸ”® å•æ ·æœ¬é¢„æµ‹...")
    sample = generate_sample_features()
    print(f"   è¾“å…¥ç‰¹å¾: {sample}")
    
    result = client.predict(
        wide_features=sample["wide"],
        deep_features=sample["deep"],
        query_hash=int(sample["query_hash"]),
        doc_hash=int(sample["doc_hash"]),
        position_group=int(sample["position_group"])
    )
    print(f"   é¢„æµ‹ç»“æœ: {result}")
    
    # 4. æ‰¹é‡é¢„æµ‹
    print("\nğŸ“¦ æ‰¹é‡é¢„æµ‹ (3ä¸ªæ ·æœ¬)...")
    batch_samples = []
    for i in range(3):
        s = generate_sample_features()
        batch_samples.append({
            "wide": s["wide"],
            "deep": s["deep"],
            "query_hash": int(s["query_hash"]),
            "doc_hash": int(s["doc_hash"]),
            "position_group": int(s["position_group"])
        })
    
    batch_result = client.batch_predict(batch_samples)
    print(f"   æ‰¹é‡é¢„æµ‹ç»“æœ: {batch_result}")
    
    # 5. æ€§èƒ½æµ‹è¯•
    print("\nâ±ï¸  æ€§èƒ½æµ‹è¯• (100æ¬¡é¢„æµ‹)...")
    import time
    start_time = time.time()
    
    for _ in range(100):
        s = generate_sample_features()
        client.predict(
            wide_features=s["wide"],
            deep_features=s["deep"],
            query_hash=int(s["query_hash"]),
            doc_hash=int(s["doc_hash"]),
            position_group=int(s["position_group"])
        )
    
    elapsed_time = time.time() - start_time
    print(f"   æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"   å¹³å‡å»¶è¿Ÿ: {elapsed_time/100*1000:.2f}ms")
    print(f"   QPS: {100/elapsed_time:.2f}")
    
    print("\n" + "=" * 60)
    print("âœ… å®¢æˆ·ç«¯ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("=" * 60)


if __name__ == '__main__':
    main()
